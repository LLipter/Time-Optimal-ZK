\chapter{Background}

% zero-knowledge in general

In cryptography, a \textbf{zero-knowledge proof} is a protocol which allow an untrusted prover $\mathcal{P}$ to convince a sceptical verifier $\mathcal{V}$ that a statement is true without revealing any further information about why the statement is true. 
Example use-cases include verifiable computing, where a powerful, but untrusted server proves, to a computationally weak client, that they performed a large calculation correctly. 
The essence of zero-knowledge proofs is that it is trivial to prove that one possesses knowledge of certain information by simply revealing it; the challenge is to prove such possession without revealing the information itself or any additional information.

% linear-time zero-knowledge

Efficiency is crucial for large and complex statements especially when we want to deploy those protocols in practice. Important efficiency parameters include but not limited to the time complexity of the prover, the time complexity of the verifier, the amount of communication measured in bits, and the number of rounds the prover and verifier need to interact. In particular, we are interested in the protocol where the prover time is linear to the size of the statement. We call such protocols \textbf{linear-time zero-knowledge protocols}.

% interactive oracle proofs
% https://www.iacr.org/archive/tcc2016b/99850156/99850156.pdf
After years of researching, lots of new proof system models have been introduced. 
One of the most well-known models is \textbf{Interactive proofs (IPs)}. Interactive proofs were introduced by Goldwasser, Micali, and Rackoff in \cite{DBLP:books/acm/19/GoldwasserMR19} decades ago, in which a probabilistic polynomial-time verifier exchanges $k$-rounds of messages with an all-powerful prover, and then accepts or rejects the statements. 

One of the most famous and the most earliest interactive proof is the quadratic residuosity problem, in which we want to decide whether $a$ is a quadratic residue mod $N$, given $N$ and $a$. For an integer $N$, we say that $a$ ($0 \le a \le N - 1$) is a quadratic residue mod $N$ if there is an $r$ (a square root) such that $ a \equiv r^2 ( \texttt{mod } N)$. 

The interactive proofs turn out to be very powerful, more and more protocols for problems in different areas have been proposed later. There even exists an interactive proof for language that may not rest in NP, for example, graph non-isomorphism (GNI) problem.

% https://en.wikipedia.org/wiki/Probabilistically_checkable_proof
% https://en.wikipedia.org/wiki/PCP_theorem#Origin_of_the_initials
Another famous model is \textbf{Probabilistically checkable proofs (PCPs)}. Probabilistically checkable proofs were introduced by \cite{DBLP:journals/tcs/FortnowRS94} \cite{DBLP:conf/stoc/BabaiFLS91}. In a probabilistically-checkable proof, a probabilistic polynomial-time verifier has oracle access to a proof string and has access to a bounded amount of randomness. The verifier is then required to accept correct proofs and reject incorrect proofs with very high probability. 

Comparing to a standard non-interactive proof where the verifier  deterministically reads the whole proof, always accepts correct proofs and rejects incorrect proofs, 
PCPs are interesting because the existence of probabilistically checkable proofs that can be checked by reading only a few bits of the proof using randomness in an essential way.

The number of queries required and the amount of randomness used are important measurements for PCPs. $\texttt{PCP}[r, q]$ is the class of languages for which the verifier uses at most $r$ bits of randomness, and queries at most $q$ locations of the proof.
In 1990 Babai, Fortnow, and Lund \cite{DBLP:conf/focs/BabaiFL90} proved that PCP[poly(n), poly(n)] = NEXP. 
Later, the PCP theorem (also known as the PCP characterization theorem), 
a major result in computational complexity theory, 
states that every decision problem in the NP complexity class has probabilistically checkable proofs of constant query complexity 
and logarithmic randomness complexity (uses a logarithmic number of random bits).
Namely, $\texttt{PCP}[O(\log n), O(1)] = \texttt{NP}$.

Also, though PCPs are protocols purely in theory due to the oracle used in the proofs, researches later have proposed methods \cite{DBLP:conf/stoc/Kilian92} to compile PCPs into argument systems that can be implemented in reality, making PCPs impactful not only in theory but also in practice. 

Later, \textbf{Interactive oracle proofs (IOPs)} was introduced by \cite{DBLP:conf/tcc/Ben-SassonCS16} and \cite{DBLP:journals/jacm/KalaiRR22}, which naturally combines aspects of IPs and PCPs and also generalizes interactive PCPs 
(which consist of a PCP followed by an IP). 
Namely, an IOP is a ``multi-round PCP'' that generalizes an interactive proof as follows:
the verifier $\mathcal{V}$ has oracle access to the prover $\mathcal{P}$’s messages, and may probabilistically query them (rather than having to read them in full). 

In more detail, a $k$-round IOP comprises $k$ rounds of interaction. In the $i$-th round of interaction: the verifier $\mathcal{V}$ sends a message $m_i$ to the prover $\mathcal{P}$, which he reads in full; then the prover $\mathcal{P}$ replies with a message $f_i$ to the verifier $\mathcal{V}$, which he can query, as an oracle proof string, in this and all later rounds. After the $k$ rounds of interaction, the verifier $\mathcal{V}$ either accepts or rejects the statement.

IOPs is more powerful because 
it retain the expressiveness of PCPs, 
capturing NEXP rather than only PSPACE,
allowing reading only a few bits of the proof,
and also the flexibility of IPs, 
allowing multiple rounds of communication with the prover. 
IOPs have already found several applications, including unconditional
zero knowledge, 
constant-rate constant-query probabilistic checking, 
and doubly-efficient constant-round IPs for polynomial-time bounded-space computations.


% polynomial commitment schemes
% https://eprint.iacr.org/2020/1536.pdf
% https://www.iacr.org/archive/asiacrypt2010/6477178/6477178.pdf
% https://www.mdpi.com/2079-9292/11/1/131/pdf#:~:text=Polynomial%20commitment%20schemes%20are%20important,at%20a%20public%20point%20later.
In this thesis, we focus on \textbf{polynomial commitment schemes}, 
initially introduced by \cite{DBLP:conf/asiacrypt/KateZG10}.
Later, many constructions 
\cite{DBLP:conf/sp/TomescuCZAPGD20} 
\cite{DBLP:journals/iacr/YurekLFKM21} 
\cite{277222}
have been proposed.
Polynomial commitment schemes are important building blocks for the construction of Succinct Non-interactive Arguments of Knowledge (SNARks), 
which is receiving a lot of attention as a core privacy-enhancing technology for blockchain applications recently.

Polynomial commitment schemes enable the prover to commit to a secret polynomial and convince the verifier that the evaluation of the committed polynomial is correct at a public point later.
Comparing to the homomorphic commitment schemes in the literature, 
whose sizes of the commitments are linear in the degree of the committed polynomial, 
that can be used to achieve the same goal,
polynomial commitment schemes are of constant size (single elements)
and the overhead of opening a commitment is also constant; even opening multiple evaluations requires only a constant amount of communication overhead. Therefore, polynomial commitment schemes are useful tools to reduce the communication cost in many cryptographic protocols.


% linear time encodable codes
% https://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2013/MSC/MSC-2013-22.pdf
% https://people.csail.mit.edu/henrycg/pers/gelfand/gelfand1973complexity.pdf
In coding theory, a linear code is an error-correcting code for which any linear combination of codewords is also a codeword. The encoding function maps vectors in space $\mathbb{F}^k$ to vectors in space $\mathbb{F}^n$, where $\mathbb{F}$ is a finite field with $q$ elements. The relative distance is defined to be the minimum distance between any valid codewords divided by $n$, the length of the codeword. 

Reed-Solomon code is one of the examples which can encode efficiently using FFT algorithm running in $O(n \log n)$ time.
Random linear code is another example.
And it is well known that a random linear code, 
has a good minimal distance with high probability.
However, one major disadvantage of random linear codes is that their encoding
complexity grows quadratically with the message length.

Those linear codes are important because many protocols rely on special families of error-correcting codes, whose properties influence the final performance of the proof systems. For example, \cite{brakedown} \cite{cryptoeprint:2020/1426} \cite{BCL22} rely on codes with a tensor structure. The lower the dimension of the tensors, the smaller the proof size and verification time of the zero-knowledge proofs.

One of the main research directions is minimizing the encoding complexity
of codes. Since the best asymptotic encoding complexity one could hope for is linear in $k$, the length of input vectors, it is natural to ask whether there are asymptotically good families of \textbf{linear-time encodable codes}.
The first proof that such codes exist is due to Gelfand, Dobrushin and Pinsker \cite{gelfand1973complexity}, who presented a randomized construction of linear-time encodable linear codes over the binary field which
have positive rate and relative minimal distance.
An explicit construction of such codes, which also admits a linear-time decoding algorithm, was given in a celebrated work of Spielman \cite{DBLP:conf/stoc/Spielman95}.

The concrete rate/distance tradeoff achieved by Spielman’s codes is far
from the GV bound. Guruswami and Indyk \cite{DBLP:journals/tit/GuruswamiI05} construct linear-time encodable codes whose rate and distance parameters can get arbitrarily close to the GV bound. 
Unfortunately, the closer one wishes to get to the bound, 
the larger the size of the underlying field becomes. 
These results leave open the existence of linear-time encodable codes which meet the GV bound, or even get close to this bound in the binary case.


% Start by gathering references
% Then fill in the new section with as much text as you can so that we can discuss it together.
% Explain what the object is. Why it is useful.
% List best papers/history and what they contribute.


% An interactive argument (or computationally sound proof system) is a relaxation of an interactive proof, introduced in \cite{10.1145/22145.22178}. The difference is that the prover is restricted to be a polynomial-time algorithm for an interactive argument, whereas no such restrictions on the prover apply for an interactive proof. In this thesis, we focus on the practical usage of \textbf{linear time} interactive argument with or without zero-knowledge property. Additionally, many interactive argument systems use polynomial commitment as a fundamental building block. And polynomial commitment can imply a general interactive argument system. Hence, we will investigate the linear time polynomial commitment instead.

%   After years of research improving the proof size and verifier run-time of zero-knowledge proofs, prover runtime remains a major bottleneck.

% A line of work \cite{brakedown} \cite{cryptoeprint:2020/1426} \cite{BCL22} attempts to address this with zero-knowledge protocols where prover runtime is a constant multiplied by the time taken to perform the calculation. The only one of these works which investigates the practical efficiency of their constructions is \cite{brakedown}, which makes stronger security assumptions than the other works to achieve zero-knowledge and good verifier runtime. On the other hand, \cite{cryptoeprint:2020/1426} \cite{BCL22} rely on specialised constructions of error correcting codes, hash functions, and sub-protocols whose practical performance is unknown.

% The protocols in these works rely on special families of linear-time error-correcting codes, whose properties influence the final performance of the proof systems. Firstly, \cite{brakedown} \cite{cryptoeprint:2020/1426} \cite{BCL22} rely on codes with a tensor structure. The lower the dimension of the tensors, the smaller the proof size and verification time of the zero-knowledge proofs. The implementation of \cite{brakedown} uses low dimension. The first goal of this project is to extend the implementation of \cite{brakedown} to higher dimension and investigate the impact of dimension on performance, at various different security levels.

% Secondly, while \cite{brakedown} rely on strong assumptions in order to achieve zero-knowledge, \cite{cryptoeprint:2020/1426} \cite{BCL22} use codes with extra zero-knowledge properties. Codes with zero-knowledge properties are easy to obtain from plain codes via simple transformations that is similar to one-time-pad encryption, and codes with stronger zero-knowledge properties use more complex transformations, as described in \cite{10.1145/2554797.2554815}. The second goal of this project is to investigate the performance impact of adding zero-knowledge to the \cite{brakedown} polynomial commitment scheme using zero-knowledge codes, using the plain codes implemented by \cite{brakedown} as a starting point, and assess which transformations are more practical.
