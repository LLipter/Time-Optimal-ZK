\chapter{Background}

In this chapter, we first explain the proof system models that will be used later in the thesis and their relevant history. Then we introduce the concept of polynomial commitment schemes and linear code. They are the essential building blocks of the cryptographic protocols studied in this thesis.

\section{Proof System Models}

% zero-knowledge in general
A \textbf{zero-knowledge proof} is a cryptographic protocol that enables an untrusted prover to convince a skeptical verifier that a statement is true without revealing any further information about how the statement is true. 
Verifiable computing is one example use-case, where a powerful, but untrusted server proves, to a computationally weak client, that they performed an extensive calculation correctly.
This technique is important and draws lots of attention because it is trivial to prove that one has knowledge of certain information by revealing it; the trick is to prove that possession without revealing the information itself or any additional information. 



% interactive oracle proofs
% https://www.iacr.org/archive/tcc2016b/99850156/99850156.pdf
Obviously, the standard mathematical proof that needs to show the solution to the verifier is not zero-knowledge.
To achieve zero-knowledge, lots of new proof system models have been introduced after years of research. One of the most well-known models is \textbf{Interactive proofs (IPs)}. Interactive proofs were proposed by Goldwasser, Micali, and Rackoff in \cite{DBLP:books/acm/19/GoldwasserMR19} decades ago, which also presents the idea of zero-knowledge for the first time. In Interactive proofs, a probabilistic polynomial-time (PPT) verifier $\mathcal{V}$ exchanges $k$-rounds of messages with an all-powerful prover $\mathcal{P}$, and then either accepts or rejects the statements.

One of the most famous and the earliest interactive proof is the quadratic residuosity problem, in which we want to decide whether $a$ is a quadratic residue mod $N$, given $N$ and $a$. For an integer $N$, we say that $a$ ($0 \le a \le N - 1$) is a quadratic residue mod $N$ if there is an $r$ (a square root) such that $ a \equiv r^2 ( \texttt{mod } N)$. 

The interactive proofs turn out to be very powerful, and more and more protocols for problems in different areas have been proposed later. There even exists an interactive proof for language that may not rest in NP, for example, graph non-isomorphism (GNI) problem.
The terminology IP also refers to the class of problems that can be solved by interactive proof systems in computational complexity theory. And one of the well-known results is that IP=PSPACE, where PSPACE denotes the class of decision problems that can is solvable by Turing machines using a polynomial amount of space.

Also, an argument is a relaxation of a proof. The interactive argument is introduced in \cite{10.1145/22145.22178}.
The difference is that the prover in the argument system is computationally bounded, whereas the prover in the proof system has no such restrictions and is assumed to have unlimited resources. This is why arguments are therefore called ``computationally sound proofs''. In the rest of this chapter, we will use the word arguments/proofs interchangeably and ignore their subtle difference.



% linear-time zero-knowledge
Efficiency is crucial for large and complex statements especially when we want to deploy those protocols in practice. Important efficiency parameters including but not limited to the time complexity of the prover $\mathcal{P}$, the time complexity of the verifier $\mathcal{V}$, the size of proof measured in bytes, and the number of rounds the prover and verifier need to communicate. In particular, we are interested in the protocol where the prover time is linear to the size of the statement. We call such protocols \textbf{linear-time zero-knowledge protocols}.


% https://en.wikipedia.org/wiki/Probabilistically_checkable_proof
% https://en.wikipedia.org/wiki/PCP_theorem#Origin_of_the_initials
Another famous model is \textbf{Probabilistically checkable proofs (PCPs)}. Probabilistically checkable proofs were proposed by \cite{DBLP:journals/tcs/FortnowRS94} \cite{DBLP:conf/stoc/BabaiFLS91}. In a probabilistically-checkable proof, a probabilistic polynomial-time (PPT) verifier $\mathcal{V}$ has oracle access to a proof string and has access to a bounded amount of randomness. The verifier $\mathcal{V}$ is then required to either accept correct proofs or reject incorrect proofs with at least a constant probability. The soundness error can be further reduced to arbitrarily small by executing the protocol multiple times.

Compared to a standard mathematical proof where the verifier $\mathcal{V}$ deterministically reads the whole proof, always accepts correct proofs, and rejects incorrect proofs, 
PCPs are interesting and powerful because of the existence of probabilistically checkable proofs that can be verified by checking only a small portion of the proof using randomness in a non-trivial way.

The number of queries made by the verifier $\mathcal{V}$ and the amount of randomness used are important measurements for PCPs. $\texttt{PCP}[r, q]$ is the class of languages for which the verifier uses at most $r$ bits of randomness, and queries at most $q$ locations of the proof string.
Babai, Fortnow, and Lund \cite{DBLP:conf/focs/BabaiFL90} proved that PCP[poly(n), poly(n)] = NEXP in 1990. 
Later, the PCP theorem (also as known as the PCP characterization theorem), 
a major result in computational complexity theory, 
states that every decision problem in the NP complexity class has probabilistically checkable proofs (PCPs) using a constant number of queries and a logarithmic number of random bits.
Namely, $\texttt{PCP}[O(\log n), O(1)] = \texttt{NP}$.

Also, though PCPs are protocols purely in theory due to the oracle used in the proofs, researchers later have proposed methods \cite{DBLP:conf/stoc/Kilian92} to compile PCPs into argument systems that can be implemented in reality, making PCPs impactful not only in theory but also in practice. 

Later, \textbf{Interactive oracle proofs (IOPs)} was introduced by \cite{DBLP:conf/tcc/Ben-SassonCS16} and \cite{DBLP:journals/jacm/KalaiRR22}, which is powerful and is the focus of this thesis. It naturally combines the structure of IPs and PCPs.
In other words, an IOP is a PCP that consists of multiple rounds. It generalizes an interactive proof as follows:
the verifier $\mathcal{V}$ has oracle access to the prover $\mathcal{P}$’s messages and may query them on a few positions probabilistically (rather than having to read the proof string in full). The IOPs with such a query pattern are also called point IOPs.

To be more precise, a $k$-round IOP consists of $k$ rounds of interaction. In the $i$-th round of interaction, the verifier $\mathcal{V}$ sends a message $m_i$ to the prover $\mathcal{P}$, which the prover $\mathcal{P}$ reads in full. 
Then the prover $\mathcal{P}$ replies with a message $o_i$ to the verifier $\mathcal{V}$ as an oracle proof string.
The verifier $\mathcal{V}$ can query $o_i$ either in this or in all later rounds. 
After the $k$ rounds of interaction, the verifier $\mathcal{V}$ either accepts or rejects the statement.
Apart from point queries, other query classes are also possible.
For example, IOPs with polynomial query patterns are called polynomial-query IOPs.

IOPs are even more powerful because, on one hand, it preserves the expressiveness and richness of PCPs, containing the complexity class NEXP rather than only PSPACE, allowing checking only a few positions of the proof string. Also, on the other hand, it is as flexible as IPs, allowing multiple rounds of interaction with the prover $\mathcal{P}$. IOPs have already found several major applications, including linear-time zero-knowledge arguments, zero-knowledge proofs for NP relations with bounded space requirements, and verifiable computing.
Additionally, lots of researchers have built linear-time IOPs, while whether linear-time PCPs with non-trivial query complexity exists is still an open question.


\section{Polynomial Commitment Scheme}

% polynomial commitment schemes
% https://eprint.iacr.org/2020/1536.pdf
% https://www.iacr.org/archive/asiacrypt2010/6477178/6477178.pdf
% https://www.mdpi.com/2079-9292/11/1/131/pdf#:~:text=Polynomial%20commitment%20schemes%20are%20important,at%20a%20public%20point%20later.
In this thesis, we focus on \textbf{polynomial commitment schemes}, 
initially introduced by \cite{DBLP:conf/asiacrypt/KateZG10}.
Later, many constructions 
\cite{DBLP:conf/sp/TomescuCZAPGD20} 
\cite{DBLP:journals/iacr/YurekLFKM21} 
\cite{277222}
have been proposed.
Polynomial commitment schemes are fundamental building blocks for the construction of Succinct Non-interactive Arguments of Knowledge (SNARks), 
which recently has received a lot of attention as a core privacy-preserving technology used in applications like blockchain. Many interesting real-world statements can be embedded into a polynomial and the proof of such a statement can be converted to an evaluation of a specific point. Also, polynomial commitments can be used to compile polynomial-query IOPs into arguments. Linear-time polynomial commitment schemes can imply a linear-time proof system.

In a polynomial commitment scheme, a prover $\mathcal{P}$ commits to a secret polynomial and convinces the verifier $\mathcal{V}$ that the evaluation result of the committed secret polynomial is correct after several rounds of communication later. Homomorphic commitment schemes in the literature, which commit to each coefficient of the polynomial, can be used to achieve the same goal. Polynomial commitment schemes are more efficient by reducing the size of the proof string significantly. For homomorphic commitment schemes, the size of the commitments is linear in the degree of the committed polynomial. For polynomial commitment schemes, the size of the commitments is constant (only a single element), and is irrelevant to the degree. And the overhead of opening a commitment could be sub-linear as well. It may also support opening multiple evaluation points with only a small amount of communication overhead. Therefore, polynomial commitment schemes are useful tools to reduce communication costs and proof size in many cryptographic protocols.

\section{Linear Code}

% linear time encodable codes
% https://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2013/MSC/MSC-2013-22.pdf
% https://people.csail.mit.edu/henrycg/pers/gelfand/gelfand1973complexity.pdf
Linear codes are essential in many proof systems. Those protocols rely on special families of error-correcting linear codes, whose structures and properties influence the overall performance of the proof systems. For instance, \cite{brakedown} \cite{cryptoeprint:2020/1426} \cite{BCL22} rely on codes with a tensor structure. The higher the dimension of the tensors, the smaller the proof size and verification time of the zero-knowledge proofs. Other proof systems that make use of linear code include \cite{DBLP:conf/coco/BordageLNR22}, \cite{DBLP:conf/icalp/Ben-SassonBHR18}, \cite{DBLP:conf/ccs/AmesHIV17} and \cite{DBLP:conf/crypto/Ben-SassonBHR19}.


In coding theory, a linear code is a mapping between vectors in space $\mathbb{F}^k$ and vectors in space $\mathbb{F}^n$, where $\mathbb{F}$ is a finite field with $q$ elements. It is an error-correcting code and it is linear because any linear combination of codewords is also a codeword. A linear code is often defined by a generator matrix $G \in \mathbb{F}^{k \times n}$. The relative distance is defined to be the minimum distance between any valid distinct codewords divided by $n$, the length of the codeword. The rate of a linear code is the ratio between the length of the message and the length of the codeword. Since we are interested in linear-time protocols, the linear code used in the protocol must also be in linear time, which implies a constant code rate ($k = O(n)$).

To encode a message, we can use the naive algorithm, where the message vector is multiplied by the generator matrix $G$ directly. Obviously, this naive algorithm runs in $O(n^2)$ time. 
Random linear code is generated by sampling the generator matrix randomly, and it is an example that will use the naive encoding algorithm. It is well known that a random linear code may have a good minimal distance property with overwhelming probability. However, one major disadvantage of random linear codes is that their encoding complexity grows quadratically with the message length. The best-known algorithm for vector-matrix multiplication is still far from linear time, which is an essential requirement for building a linear-time zero-knowledge proof system that uses linear code. For other types of code that have a special structure, faster algorithms are possible. Reed-Solomon code is one such example, whose generator matrix is a Vandermonde matrix It can encode the message in $O(n \log n)$ time with the help of the FFT algorithm.



One of the main research directions is minimizing the encoding complexity
of codes. Since the reading of the input message already takes linear time, the best asymptotic encoding time complexity one could imagine is linear in $k$, the size of the input vectors. One question to ask is whether it is possible to find a family of efficient \textbf{linear-time encodable codes}. Gelfand, Dobrushin, and Pinsker \cite{gelfand1973complexity} presented a first proof showing a positive answer. They presented a randomized construction of linear-time encodable linear codes over the binary field. Later Spielman \cite{DBLP:conf/stoc/Spielman95} gives an explicit construction of such codes, which includes both a linear-time encoding algorithm and a linear-time decoding algorithm. The Brakedown linear code presented in \cite{brakedown} recently is another example.

Another research direction is maximizing the minimum distance property. Minimum distance property can directly affect the soundness error of many cryptographic protocols including the polynomial commitment schemes focus on by this thesis. Generally speaking, the larger the minimum distance, the smaller the soundness error, and smaller security parameters and proof size may be achievable. Since the concrete rate/distance tradeoff obtained by Spielman’s codes is far from the theoretical GV bound, lots of work has been done to improve that. For example, Guruswami and Indyk \cite{DBLP:journals/tit/GuruswamiI05} introduced linear-time encodable codes whose rate and distance property can get arbitrarily close to the GV bound. And Druk and Ishai \cite{10.1145/2554797.2554815} proposed a construction to further boost the relative distance property. 
